{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Statistcal Language Model Trained on Charles Dicken's A Christmas Carol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = open('christmascarol.txt', 'r')\n",
    "text = file.read()\n",
    "file.close()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "In Prose\n\nBEING A GHOST STORY OF CHRISTMAS\n\n\n\n\nSTAVE ONE\n\nMARLEY'S GHOST\n\n\nMarley was dead, to begin with. There is no doubt whatever about that.\nThe register of his burial was signed by the clergyman, the clerk, the\nundertaker, and the chief mourner. Scrooge signed it. And Scrooge's name\nwas good u\n"
    }
   ],
   "source": [
    "print(text[:300])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Replace ‘–‘ with a white space so we can split words better.\n",
    "* Split words based on white space.\n",
    "* Remove all punctuation from words to reduce the vocabulary size (e.g. ‘What?’ becomes ‘What’).\n",
    "* Remove all words that are not alphabetic to remove standalone punctuation tokens.\n",
    "* Normalize all words to lowercase to reduce the vocabulary size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "28834\n"
    }
   ],
   "source": [
    "import string\n",
    "\n",
    "def staging(text):\n",
    "    text = text.replace('-', ' ')\n",
    "    tokens = text.split()\n",
    "\n",
    "    # remove punctuation from each token\n",
    "    p_map = string.punctuation.maketrans('','','.') #save fulstop\n",
    "    p_remove = string.punctuation.translate(p_map)\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "\n",
    "\t# remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "\t# make lower case\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "\n",
    "\n",
    "    return tokens\n",
    "\n",
    "tokens = staging(text)\n",
    "print(len(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Number of sequences to avail:  28783\n"
    }
   ],
   "source": [
    "seq_len = 51\n",
    "seq_all = []\n",
    "\n",
    "for i in range(seq_len , len(tokens)):\n",
    "    seq = tokens[i - seq_len : i]\n",
    "    line = ' '.join(seq)\n",
    "    seq_all.append(line)\n",
    "print(\"Number of sequences to avail: \", len(seq_all))\n",
    "\n",
    "\n",
    "#save to file\n",
    "data = '\\n'.join(seq_all)\n",
    "file = open('data_in.txt', 'w')\n",
    "file.write(data)\n",
    "file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load file into memory\n",
    "\n",
    "def load_doc(filename):\n",
    "    file = open(filename, 'r')\n",
    "    text = file.read()\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "filename_0 = 'data_in.txt'\n",
    "content = load_doc(filename_0)\n",
    "lines = content.split('\\n')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(lines)\n",
    "sequences = tokenizer.texts_to_sequences(lines)\n",
    "vocab_size = len(tokenizer.word_index) + 1   #non-zero offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "\n",
    "# separate into input and output\n",
    "sequences = np.array(sequences)\n",
    "X, y = sequences[:,:-1], sequences[:,-1]\n",
    "y = to_categorical(y, num_classes=vocab_size)\n",
    "seq_length = X.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Model: \"sequential_1\"\n_________________________________________________________________\nLayer (type)                 Output Shape              Param #   \n=================================================================\nembedding_1 (Embedding)      (None, 50, 50)            214150    \n_________________________________________________________________\nlstm_1 (LSTM)                (None, 50, 100)           60400     \n_________________________________________________________________\nlstm_2 (LSTM)                (None, 100)               80400     \n_________________________________________________________________\ndense_1 (Dense)              (None, 100)               10100     \n_________________________________________________________________\ndense_2 (Dense)              (None, 4283)              432583    \n=================================================================\nTotal params: 797,633\nTrainable params: 797,633\nNon-trainable params: 0\n_________________________________________________________________\nNone\nEpoch 1/100\n28783/28783 [==============================] - 23s 784us/step - loss: 6.7540 - accuracy: 0.0536\nEpoch 2/100\n28783/28783 [==============================] - 22s 764us/step - loss: 6.3100 - accuracy: 0.0544\nEpoch 3/100\n28783/28783 [==============================] - 22s 750us/step - loss: 6.1621 - accuracy: 0.0582\nEpoch 4/100\n28783/28783 [==============================] - 21s 738us/step - loss: 6.0059 - accuracy: 0.0669\nEpoch 5/100\n28783/28783 [==============================] - 21s 746us/step - loss: 5.8947 - accuracy: 0.0696\nEpoch 6/100\n28783/28783 [==============================] - 21s 739us/step - loss: 5.8132 - accuracy: 0.0708\nEpoch 7/100\n28783/28783 [==============================] - 21s 745us/step - loss: 5.7403 - accuracy: 0.0766\nEpoch 8/100\n28783/28783 [==============================] - 21s 727us/step - loss: 5.6612 - accuracy: 0.0814\nEpoch 9/100\n28783/28783 [==============================] - 21s 745us/step - loss: 5.5698 - accuracy: 0.0897\nEpoch 10/100\n28783/28783 [==============================] - 22s 748us/step - loss: 5.4857 - accuracy: 0.0968\nEpoch 11/100\n28783/28783 [==============================] - 21s 743us/step - loss: 5.4098 - accuracy: 0.1031\nEpoch 12/100\n28783/28783 [==============================] - 24s 847us/step - loss: 5.3354 - accuracy: 0.1052\nEpoch 13/100\n28783/28783 [==============================] - 22s 766us/step - loss: 5.2625 - accuracy: 0.1080\nEpoch 14/100\n28783/28783 [==============================] - 22s 764us/step - loss: 5.1915 - accuracy: 0.1107\nEpoch 15/100\n28783/28783 [==============================] - 23s 782us/step - loss: 5.1239 - accuracy: 0.1147\nEpoch 16/100\n28783/28783 [==============================] - 22s 751us/step - loss: 5.0597 - accuracy: 0.1159\nEpoch 17/100\n28783/28783 [==============================] - 22s 780us/step - loss: 4.9960 - accuracy: 0.1185\nEpoch 18/100\n28783/28783 [==============================] - 23s 811us/step - loss: 4.9352 - accuracy: 0.1209\nEpoch 19/100\n28783/28783 [==============================] - 24s 817us/step - loss: 4.8740 - accuracy: 0.1241\nEpoch 20/100\n28783/28783 [==============================] - 23s 783us/step - loss: 4.8167 - accuracy: 0.1250\nEpoch 21/100\n28783/28783 [==============================] - 21s 733us/step - loss: 4.7597 - accuracy: 0.1285\nEpoch 22/100\n28783/28783 [==============================] - 21s 734us/step - loss: 4.7022 - accuracy: 0.1292\nEpoch 23/100\n28783/28783 [==============================] - 21s 743us/step - loss: 4.6491 - accuracy: 0.1340\nEpoch 24/100\n28783/28783 [==============================] - 21s 734us/step - loss: 4.5938 - accuracy: 0.1366\nEpoch 25/100\n28783/28783 [==============================] - 21s 734us/step - loss: 4.5430 - accuracy: 0.1366\nEpoch 26/100\n28783/28783 [==============================] - 21s 741us/step - loss: 4.4890 - accuracy: 0.1380\nEpoch 27/100\n28783/28783 [==============================] - 21s 734us/step - loss: 4.4381 - accuracy: 0.1421\nEpoch 28/100\n28783/28783 [==============================] - 21s 737us/step - loss: 4.3889 - accuracy: 0.1436\nEpoch 29/100\n28783/28783 [==============================] - 21s 742us/step - loss: 4.3374 - accuracy: 0.1461\nEpoch 30/100\n28783/28783 [==============================] - 21s 739us/step - loss: 4.2891 - accuracy: 0.1500\nEpoch 31/100\n28783/28783 [==============================] - 21s 739us/step - loss: 4.2414 - accuracy: 0.1505\nEpoch 32/100\n28783/28783 [==============================] - 22s 749us/step - loss: 4.1912 - accuracy: 0.1552\nEpoch 33/100\n28783/28783 [==============================] - 21s 738us/step - loss: 4.1429 - accuracy: 0.1592\nEpoch 34/100\n28783/28783 [==============================] - 21s 742us/step - loss: 4.0917 - accuracy: 0.1625\nEpoch 35/100\n28783/28783 [==============================] - 22s 754us/step - loss: 4.0472 - accuracy: 0.1671\nEpoch 36/100\n28783/28783 [==============================] - 22s 750us/step - loss: 4.0040 - accuracy: 0.1699\nEpoch 37/100\n28783/28783 [==============================] - 21s 742us/step - loss: 3.9565 - accuracy: 0.1747\nEpoch 38/100\n28783/28783 [==============================] - 22s 766us/step - loss: 3.9120 - accuracy: 0.1791\nEpoch 39/100\n28783/28783 [==============================] - 22s 747us/step - loss: 3.8706 - accuracy: 0.1838\nEpoch 40/100\n28783/28783 [==============================] - 21s 731us/step - loss: 3.8291 - accuracy: 0.1902\nEpoch 41/100\n28783/28783 [==============================] - 21s 737us/step - loss: 3.7879 - accuracy: 0.1938\nEpoch 42/100\n28783/28783 [==============================] - 21s 734us/step - loss: 3.7481 - accuracy: 0.1989\nEpoch 43/100\n28783/28783 [==============================] - 21s 729us/step - loss: 3.7070 - accuracy: 0.2028\nEpoch 44/100\n28783/28783 [==============================] - 21s 729us/step - loss: 3.6672 - accuracy: 0.2066\nEpoch 45/100\n28783/28783 [==============================] - 21s 729us/step - loss: 3.6336 - accuracy: 0.2109\nEpoch 46/100\n28783/28783 [==============================] - 21s 727us/step - loss: 3.5949 - accuracy: 0.2192\nEpoch 47/100\n28783/28783 [==============================] - 21s 741us/step - loss: 3.5615 - accuracy: 0.2232\nEpoch 48/100\n28783/28783 [==============================] - 21s 738us/step - loss: 3.5265 - accuracy: 0.2269\nEpoch 49/100\n28783/28783 [==============================] - 21s 733us/step - loss: 3.4919 - accuracy: 0.2340\nEpoch 50/100\n28783/28783 [==============================] - 22s 753us/step - loss: 3.4607 - accuracy: 0.2372\nEpoch 51/100\n28783/28783 [==============================] - 22s 748us/step - loss: 3.4294 - accuracy: 0.2416\nEpoch 52/100\n28783/28783 [==============================] - 23s 784us/step - loss: 3.4540 - accuracy: 0.2378\nEpoch 53/100\n28783/28783 [==============================] - 23s 805us/step - loss: 3.3760 - accuracy: 0.2504\nEpoch 54/100\n28783/28783 [==============================] - 22s 749us/step - loss: 3.3434 - accuracy: 0.2560\nEpoch 55/100\n28783/28783 [==============================] - 22s 751us/step - loss: 3.3135 - accuracy: 0.2593\nEpoch 56/100\n28783/28783 [==============================] - 21s 739us/step - loss: 3.2876 - accuracy: 0.2654\nEpoch 57/100\n28783/28783 [==============================] - 21s 740us/step - loss: 3.2586 - accuracy: 0.2705\nEpoch 58/100\n28783/28783 [==============================] - 22s 748us/step - loss: 3.2283 - accuracy: 0.2737\nEpoch 59/100\n28783/28783 [==============================] - 22s 749us/step - loss: 3.2043 - accuracy: 0.2784\nEpoch 60/100\n28783/28783 [==============================] - 21s 738us/step - loss: 3.1757 - accuracy: 0.2814\nEpoch 61/100\n28783/28783 [==============================] - 21s 743us/step - loss: 3.1511 - accuracy: 0.2878\nEpoch 62/100\n28783/28783 [==============================] - 22s 756us/step - loss: 3.1233 - accuracy: 0.2908\nEpoch 63/100\n28783/28783 [==============================] - 22s 772us/step - loss: 3.1004 - accuracy: 0.2966\nEpoch 64/100\n28783/28783 [==============================] - 22s 765us/step - loss: 3.0749 - accuracy: 0.3011\nEpoch 65/100\n28783/28783 [==============================] - 22s 758us/step - loss: 3.0510 - accuracy: 0.3031\nEpoch 66/100\n28783/28783 [==============================] - 22s 758us/step - loss: 3.0271 - accuracy: 0.3090\nEpoch 67/100\n28783/28783 [==============================] - 24s 826us/step - loss: 3.0042 - accuracy: 0.3096\nEpoch 68/100\n28783/28783 [==============================] - 24s 826us/step - loss: 2.9826 - accuracy: 0.3148\nEpoch 69/100\n28783/28783 [==============================] - 23s 782us/step - loss: 2.9622 - accuracy: 0.3201\nEpoch 70/100\n28783/28783 [==============================] - 22s 762us/step - loss: 2.9373 - accuracy: 0.3256\nEpoch 71/100\n28783/28783 [==============================] - 23s 782us/step - loss: 2.9159 - accuracy: 0.3291\nEpoch 72/100\n28783/28783 [==============================] - 22s 765us/step - loss: 2.8957 - accuracy: 0.3301\nEpoch 73/100\n28783/28783 [==============================] - 22s 768us/step - loss: 2.8707 - accuracy: 0.3370\nEpoch 74/100\n28783/28783 [==============================] - 22s 751us/step - loss: 2.8532 - accuracy: 0.3395\nEpoch 75/100\n28783/28783 [==============================] - 22s 777us/step - loss: 2.8342 - accuracy: 0.3436\nEpoch 76/100\n28783/28783 [==============================] - 22s 775us/step - loss: 2.8108 - accuracy: 0.3460\nEpoch 77/100\n28783/28783 [==============================] - 23s 792us/step - loss: 2.7940 - accuracy: 0.3498\nEpoch 78/100\n28783/28783 [==============================] - 22s 751us/step - loss: 2.7754 - accuracy: 0.3508\nEpoch 79/100\n28783/28783 [==============================] - 21s 737us/step - loss: 2.7532 - accuracy: 0.3554\nEpoch 80/100\n28783/28783 [==============================] - 23s 803us/step - loss: 2.7299 - accuracy: 0.3625\nEpoch 81/100\n28783/28783 [==============================] - 22s 765us/step - loss: 2.7082 - accuracy: 0.3663\nEpoch 82/100\n28783/28783 [==============================] - 22s 763us/step - loss: 2.6941 - accuracy: 0.3711\nEpoch 83/100\n28783/28783 [==============================] - 21s 730us/step - loss: 2.6738 - accuracy: 0.3744\nEpoch 84/100\n28783/28783 [==============================] - 22s 763us/step - loss: 2.6535 - accuracy: 0.3777\nEpoch 85/100\n28783/28783 [==============================] - 22s 762us/step - loss: 2.6410 - accuracy: 0.3794\nEpoch 86/100\n28783/28783 [==============================] - 21s 739us/step - loss: 2.6222 - accuracy: 0.3810\nEpoch 87/100\n28783/28783 [==============================] - 21s 741us/step - loss: 2.5990 - accuracy: 0.3871\nEpoch 88/100\n28783/28783 [==============================] - 21s 738us/step - loss: 2.5839 - accuracy: 0.3906\nEpoch 89/100\n28783/28783 [==============================] - 21s 735us/step - loss: 2.5682 - accuracy: 0.3921\nEpoch 90/100\n28783/28783 [==============================] - 22s 769us/step - loss: 2.5509 - accuracy: 0.3954\nEpoch 91/100\n28783/28783 [==============================] - 25s 871us/step - loss: 2.5329 - accuracy: 0.3995\nEpoch 92/100\n28783/28783 [==============================] - 24s 847us/step - loss: 2.5128 - accuracy: 0.4042\nEpoch 93/100\n28783/28783 [==============================] - 23s 813us/step - loss: 2.4966 - accuracy: 0.4065\nEpoch 94/100\n28783/28783 [==============================] - 24s 841us/step - loss: 2.4846 - accuracy: 0.4097\nEpoch 95/100\n28783/28783 [==============================] - 24s 835us/step - loss: 2.4669 - accuracy: 0.4115\nEpoch 96/100\n28783/28783 [==============================] - 22s 764us/step - loss: 2.4465 - accuracy: 0.4180\nEpoch 97/100\n28783/28783 [==============================] - 22s 748us/step - loss: 2.4336 - accuracy: 0.4188\nEpoch 98/100\n28783/28783 [==============================] - 23s 812us/step - loss: 2.4195 - accuracy: 0.4233\nEpoch 99/100\n28783/28783 [==============================] - 29s 996us/step - loss: 2.4024 - accuracy: 0.4250\nEpoch 100/100\n28783/28783 [==============================] - 26s 895us/step - loss: 2.3914 - accuracy: 0.4289\n"
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": "<keras.callbacks.callbacks.History at 0x7f47c8018a60>"
     },
     "metadata": {},
     "execution_count": 46
    }
   ],
   "source": [
    "\n",
    "# define model\n",
    "model = Sequential()\n",
    "model.add(Embedding(vocab_size, 50, input_length=seq_length))\n",
    "model.add(LSTM(100, return_sequences=True))\n",
    "model.add(LSTM(100))\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(vocab_size, activation='softmax'))\n",
    "print(model.summary())\n",
    "\n",
    "\n",
    "# compile model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "# fit model\n",
    "model.fit(X, y, batch_size=128, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pickle import dump, load\n",
    "\n",
    "# save the model \n",
    "model.save('model.h5')\n",
    "# save the tokenizer as pkl\n",
    "dump(tokenizer, open('tokenizer.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model and tokenizer\n",
    "\n",
    "model = load_model('model.h5')\n",
    "tokenizer = load(open('tokenizer.pkl', 'rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python38264bit8f1b4bc6f2ba477b8ca9980dae4c5a53",
   "display_name": "Python 3.8.2 64-bit"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}